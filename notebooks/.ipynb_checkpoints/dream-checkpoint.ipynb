{
 "metadata": {
  "colabVersion": "0.3.1",
  "default_view": {},
  "name": "",
  "signature": "sha256:1507463198b3308d683079df07df9c4069939672031842a975f7c649486535ac",
  "views": {}
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "RMhGdYHuOZM8"
     },
     "source": [
      "# Deep Dreams (with Caffe)\n",
      "\n",
      "This notebook demonstrates how to use [Caffe](http://caffe.berkeleyvision.org/) neural network framework to produce \"dream\" visuals shown in the [Google Research blog post](http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html).\n",
      "\n",
      "It'll be interesting to see what imagery people are able to generate using the described technique. If you post images to Google+, Facebook, or Twitter, be sure to tag them with **#deepdream** so other researchers can check them out too.\n",
      "\n",
      "##Dependences\n",
      "This notebook is designed to have as few depencences as possible:\n",
      "* Standard Python scientific stack: [NumPy](http://www.numpy.org/), [SciPy](http://www.scipy.org/), [PIL](http://www.pythonware.com/products/pil/), [IPython](http://ipython.org/). Those libraries can also be installed as a part of one of scientific packages for Python, such as [Anaconda](http://continuum.io/downloads) or [Canopy](https://store.enthought.com/).\n",
      "* [Caffe](http://caffe.berkeleyvision.org/) deep learning framework ([installation instructions](http://caffe.berkeleyvision.org/installation.html)).\n",
      "* Google [protobuf](https://developers.google.com/protocol-buffers/) library that is used for Caffe model manipulation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# imports and basic notebook setup\n",
      "from cStringIO import StringIO\n",
      "import numpy as np\n",
      "import scipy.ndimage as nd\n",
      "import PIL.Image\n",
      "from IPython.display import clear_output, Image, display\n",
      "from google.protobuf import text_format\n",
      "\n",
      "import caffe\n",
      "\n",
      "def showarray(a, fmt='jpeg'):\n",
      "    a = np.uint8(np.clip(a, 0, 255))\n",
      "    f = StringIO()\n",
      "    PIL.Image.fromarray(a).save(f, fmt)\n",
      "    display(Image(data=f.getvalue()))"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "id": "Pqz5k4syOZNA"
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "AeF9mG-COZNE"
     },
     "source": [
      "## Loading DNN model\n",
      "In this notebook we are going to use a [GoogLeNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet) model trained on [ImageNet](http://www.image-net.org/) dataset.\n",
      "Feel free to experiment with other models from Caffe [Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo). One particulary interesting [model](http://places.csail.mit.edu/downloadCNN.html) was trained in [MIT Places](http://places.csail.mit.edu/) dataset. It produced many visuals from the [original blog post](http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model_path = '/caffe-master/models/pose_estimation/'#bvlc_googlenet/'pose_estimation # substitute your path here\n",
      "net_fn   = model_path + 'mpii.prototxt'#deploy.prototxt'mpii\n",
      "param_fn = model_path + 'pose_iter_40000.caffemodel'#bvlc_googlenet.caffemodel'pose_iter_40000\n",
      "\n",
      "# Patching model to be able to compute gradients.\n",
      "# Note that you can also manually add \"force_backward: true\" line to \"deploy.prototxt\".\n",
      "model = caffe.io.caffe_pb2.NetParameter()\n",
      "text_format.Merge(open(net_fn).read(), model)\n",
      "model.force_backward = True\n",
      "open('tmp.prototxt', 'w').write(str(model))\n",
      "\n",
      "net = caffe.Classifier('tmp.prototxt', param_fn,\n",
      "                       mean = np.float32([104.0, 116.0, 122.0]), # ImageNet mean, training set dependent\n",
      "                       channel_swap = (2,1,0)) # the reference model has channels in BGR order instead of RGB\n",
      "    \n",
      "# a couple of utility functions for converting to and from Caffe's input image layout\n",
      "def preprocess(net, img):\n",
      "    print np.float32(np.rollaxis(img, 2)[::-1]).shape # printing shape for clarity\n",
      "    return np.float32(np.rollaxis(img, 2)[::-1]) # - net.transformer.mean['data']\n",
      "def deprocess(net, img):\n",
      "    return np.dstack(img) # + net.transformer.mean['data'])[::-1])"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "id": "i9hkSm1IOZNR"
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "colab_type": "text",
      "id": "UeV_fJ4QOZNb"
     },
     "source": [
      " Producing dreams"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "9udrp3efOZNd"
     },
     "source": [
      "Making the \"dream\" images is very simple. Essentially it is just a gradient ascent process that tries to maximize the L2 norm of activations of a particular DNN layer. Here are a few simple tricks that we found useful for getting good images:\n",
      "* offset image by a random jitter\n",
      "* normalize the magnitude of gradient ascent steps\n",
      "* apply ascent across multiple scales (octaves)\n",
      "\n",
      "First we implement a basic gradient ascent step function, applying the first two tricks:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_step(net, step_size=1.5, end='conv4_total', jitter=32, clip=False): #full1 inception_4c/output\n",
      "    '''Basic gradient ascent step.'''\n",
      "\n",
      "    src = net.blobs['data'] # input image is stored in Net's 'data' blob\n",
      "    dst = net.blobs[end]\n",
      "\n",
      "    ox, oy = np.random.randint(-jitter, jitter+1, 2)\n",
      "    src.data[0] = np.roll(np.roll(src.data[0], ox, -1), oy, -2) # apply jitter shift\n",
      "            \n",
      "    net.forward(end=end)\n",
      "    dst.diff[:] = dst.data  # specify the optimization objective\n",
      "    net.backward(start=end)\n",
      "    g = src.diff[0]\n",
      "    # apply normalized ascent step to the input image\n",
      "    src.data[:] += step_size/np.abs(g).mean() * g\n",
      "\n",
      "    src.data[0] = np.roll(np.roll(src.data[0], -ox, -1), -oy, -2) # unshift image\n",
      "            \n",
      "    if clip:\n",
      "        bias = 0#net.transformer.mean['data']\n",
      "        src.data[:] = np.clip(src.data, -bias, 255-bias)    "
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "id": "pN43nMsHOZNg"
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "nphEdlBgOZNk"
     },
     "source": [
      "Next we implement an ascent through different scales. We call these scales \"octaves\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def deepdream(net, base_img, iter_n=10, octave_n=4, octave_scale=1.4, end='conv4_total', clip=True, **step_params): #full1 inception_4c/output\n",
      "    # prepare base images for all octaves\n",
      "    octaves = [preprocess(net, base_img)]\n",
      "    for i in xrange(octave_n-1):\n",
      "        octaves.append(nd.zoom(octaves[-1], (1, 1.0/octave_scale,1.0/octave_scale), order=1))\n",
      "    \n",
      "    src = net.blobs['data']\n",
      "    detail = np.zeros_like(octaves[-1]) # allocate image for network-produced details\n",
      "    for octave, octave_base in enumerate(octaves[::-1]):\n",
      "        h, w = octave_base.shape[-2:]\n",
      "        if octave > 0:\n",
      "            # upscale details from the previous octave\n",
      "            h1, w1 = detail.shape[-2:]\n",
      "            detail = nd.zoom(detail, (1, 1.0*h/h1,1.0*w/w1), order=1)\n",
      "\n",
      "        print(\"in deep dream about to resize\")\n",
      "        print src.data.shape\n",
      "        print detail.shape\n",
      "        print octave_base.shape\n",
      "        src.reshape(1,3,h,w) # resize the network's input image size\n",
      "        src.data[0] = octave_base+detail\n",
      "        for i in xrange(iter_n):\n",
      "            make_step(net, end=end, clip=clip, **step_params)\n",
      "            \n",
      "            # visualization\n",
      "            vis = deprocess(net, src.data[0])\n",
      "            if not clip: # adjust image contrast if clipping is disabled\n",
      "                vis = vis*(255.0/np.percentile(vis, 99.98))\n",
      "            showarray(vis)\n",
      "            print octave, i, end, vis.shape\n",
      "            clear_output(wait=True)\n",
      "            \n",
      "        # extract details produced on the current octave\n",
      "        detail = src.data[0]-octave_base\n",
      "    # returning the resulting image\n",
      "    return deprocess(net, src.data[0])"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "id": "ZpFIn8l0OZNq"
     },
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "QrcdU-lmOZNx"
     },
     "source": [
      "Now we are ready to let the neural network to reveal its dreams! Let's take a [cloud image](https://commons.wikimedia.org/wiki/File:Appearance_of_sky_for_weather_forecast,_Dhaka,_Bangladesh.JPG) as a starting point:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#with_alpha = PIL.Image.open('/me.jpg')\n",
      "#with_alpha.load() # required for png.split()\n",
      "\n",
      "#without_alpha = PIL.Image.new(\"RGB\", with_alpha.size, (255, 255, 255))\n",
      "#without_alpha.paste(with_alpha, mask=with_alpha.split()[3]) # 3 is the alpha channel\n",
      "\n",
      "#without_alpha.save('/src/newinput.jpg', 'JPEG', quality=80)\n",
      "\n",
      "#img = np.float32(PIL.Image.open('/input/me.jpg'))\n",
      "\n",
      "basewidth = 128\n",
      "img = PIL.Image.open('/input/me.jpg')\n",
      "print img\n",
      "\n",
      "wpercent = (basewidth / float(img.size[0]))\n",
      "hsize = int((float(img.size[1]) * float(wpercent)))\n",
      "img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)\n",
      "img.save('/input/resized_me.jpg')\n",
      "print img\n",
      "print img\n",
      "\n",
      "img = np.float32(PIL.Image.open('/input/resized_me.jpg'))\n",
      "\n",
      "showarray(img)"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "executionInfo": null,
      "id": "40p5AqqwOZN5",
      "outputId": "f62cde37-79e8-420a-e448-3b9b48ee1730",
      "pinned": false
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=960x720 at 0x7FADAAE30EF0>\n",
        "<PIL.Image.Image image mode=RGB size=128x96 at 0x7FADAAE30D88>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "<PIL.Image.Image image mode=RGB size=128x96 at 0x7FADAAE30D88>\n"
       ]
      },
      {
       "jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\n",
       "metadata": {},
       "output_type": "display_data",
       "text": [
        "<IPython.core.display.Image at 0x7fad95258b50>"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "Z9_215_GOZOL"
     },
     "source": [
      "Running the next code cell starts the detail generation process. You may see how new patterns start to form, iteration by iteration, octave by octave."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_=deepdream(net, img)"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "executionInfo": null,
      "id": "HlnVnDTlOZOL",
      "outputId": "425dfc83-b474-4a69-8386-30d86361bbf6",
      "pinned": false
     },
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'deepdream' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-d4150d0aed19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepdream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'deepdream' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "Rp9kOCQTOZOQ"
     },
     "source": [
      "The complexity of the details generated depends on which layer's activations we try to maximize. Higher layers produce complex features, while lower ones enhance edges and textures, giving the image an impressionist feeling:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_=deepdream(net, img, end='inception_3b/5x5_reduce')"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "executionInfo": null,
      "id": "eHOX0t93OZOR",
      "outputId": "0de0381c-4681-4619-912f-9b6a2cdec0c6",
      "pinned": false
     },
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'deepdream' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-ecaf682acc96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepdream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inception_3b/5x5_reduce'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mNameError\u001b[0m: name 'deepdream' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "rkzHz9E8OZOb"
     },
     "source": [
      "We encourage readers to experiment with layer selection to see how it affects the results. Execute the next code cell to see the list of different layers. You can modify the `make_step` function to make it follow some different objective, say to select a subset of activations to maximize, or to maximize multiple layers at once. There is a huge design space to explore!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "net.blobs.keys()"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "id": "OIepVN6POZOc"
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "['data',\n",
        " 'conv1',\n",
        " 'conv1_relu1_0_split_0',\n",
        " 'conv1_relu1_0_split_1',\n",
        " 'conv2_h',\n",
        " 'conv2_h_relu2_h_0_split_0',\n",
        " 'conv2_h_relu2_h_0_split_1',\n",
        " 'conv3_h',\n",
        " 'conv3b_h',\n",
        " 'conv4_h',\n",
        " 'pool2_m',\n",
        " 'conv3_m',\n",
        " 'conv3b_m',\n",
        " 'conv4_m',\n",
        " 'med_up',\n",
        " 'pool1_l',\n",
        " 'conv2_l',\n",
        " 'pool2_l',\n",
        " 'conv3_l',\n",
        " 'conv3b_l',\n",
        " 'conv4_l',\n",
        " 'low_up',\n",
        " 'conv4_total',\n",
        " 'full1',\n",
        " 'full2',\n",
        " 'full3',\n",
        " 'full3_drop_f3_0_split_0',\n",
        " 'full3_drop_f3_0_split_1',\n",
        " 'parts',\n",
        " 'seg',\n",
        " 'parts_sig',\n",
        " 'seg_sig',\n",
        " 'rankle',\n",
        " 'rknee',\n",
        " 'rhip',\n",
        " 'lhip',\n",
        " 'lknee',\n",
        " 'lankle',\n",
        " 'pelvis',\n",
        " 'thorax',\n",
        " 'neck',\n",
        " 'head',\n",
        " 'rwri',\n",
        " 'relb',\n",
        " 'rsho',\n",
        " 'lsho',\n",
        " 'lelb',\n",
        " 'lwri',\n",
        " 'sil',\n",
        " 'arm',\n",
        " 'leg']"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "vs2uUpMCOZOe"
     },
     "source": [
      "What if we feed the `deepdream` function its own output, after applying a little zoom to it? It turns out that this leads to an endless stream of impressions of the things that the network saw during training. Some patterns fire more often than others, suggestive of basins of attraction.\n",
      "\n",
      "We will start the process from the same sky image as above, but after some iteration the original image becomes irrelevant; even random noise can be used as the starting point."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!mkdir frames\n",
      "frame = img\n",
      "frame_i = 0"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "id": "IB48CnUfOZOe"
     },
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h, w = frame.shape[:2]\n",
      "s = 0.05 # scale coefficient\n",
      "for i in xrange(100):\n",
      "    frame = deepdream(net, frame)\n",
      "    PIL.Image.fromarray(np.uint8(frame)).save(\"frames/%04d.jpg\"%frame_i)\n",
      "    frame = nd.affine_transform(frame, [1-s,1-s,1], [h*s/2,w*s/2,0], order=1)\n",
      "    frame_i += 1"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "id": "fj0E-fKDOZOi"
     },
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-79-df3f7df35201>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m \u001b[1;31m# scale coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepdream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mPIL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"frames/%04d.jpg\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mframe_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maffine_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-69-975ec2ad7030>\u001b[0m in \u001b[0;36mdeepdream\u001b[1;34m(net, base_img, iter_n, octave_n, octave_scale, end, clip, **step_params)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moctave_base\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdetail\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mmake_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mstep_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m# visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-68-51f962bde3b0>\u001b[0m in \u001b[0;36mmake_step\u001b[1;34m(net, step_size, end, jitter, clip)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# apply jitter shift\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m  \u001b[1;31m# specify the optimization objective\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/caffe-master/distribute/python/caffe/pycaffe.py\u001b[0m in \u001b[0;36m_Net_forward\u001b[1;34m(self, blobs, start, end, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0min_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;31m# Unpack blobs to extract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "colab_type": "text",
      "id": "XzZGGME_OZOk"
     },
     "source": [
      "Be careful running the code above, it can bring you into very strange realms!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(filename='frames/0029.jpg')"
     ],
     "language": "python",
     "metadata": {
      "cellView": "both",
      "colab_type": "code",
      "executionInfo": null,
      "id": "ZCZcz2p1OZOt",
      "outputId": "d3773436-2b5d-4e79-be9d-0f12ab839fff",
      "pinned": false
     },
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}